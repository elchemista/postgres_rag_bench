# Search Strategies Overview

This document summarizes the different search strategies implemented in **HybridSearch** and how
they behave in practice. The goal is to understand when to reach for BM25, dense vector
similarity, or binary vector techniques.

## 1. BM25 Full-Text Search

*Implementation*: PostgreSQL `tsvector` + `ts_rank_cd`

*What it does*: Tokenizes the content of each chunk into a text vector and ranks results with a
BM25 variant. Works best for lexical matches and provides highlighted snippets via
`ts_headline/3`.

*When to use*: Traditional keyword search, TF-IDF style applications, or when exact term
matching matters (e.g., documentation lookup).

## 2. Dense Vector Search (`embedding` column)

Dense embeddings come from Bumblebee’s `thenlper/gte-small` model. They capture semantic meaning
and allow fuzzy matching beyond keywords.

### Cosine distance (`<=>`)

*Implementation*: `SELECT … ORDER BY embedding <=> query_vector`

*What it does*: Measures the cosine distance between vectors. Lower values mean higher
similarity.

*When to use*: General-purpose semantic search. Works well when vectors are normalized or when you
value angular similarity.

### L2 distance (`<->`)

*Implementation*: Euclidean distance via `embedding <-> query_vector`

*What it does*: Penalizes large coordinate differences more heavily; distance grows quadratically.

*When to use*: When magnitude differences carry meaning or when embeddings are not normalized.

### L1 distance (`<+>`)

*Implementation*: Manhattan distance via `embedding <+> query_vector`

*What it does*: Counts absolute coordinate differences; more robust to outliers than L2.

*When to use*: If you want a “taxicab” distance that is less sensitive to large deviations in a few
dimensions. In our benchmarks this is the fastest vector search because `pgvector` implements it as a
standard indexable operator (no extra normalization).

### Inner product (`<#>`)

*Implementation*: Inner product via `embedding <#> query_vector`

*What it does*: Computes the (negative) dot product; higher scores mean better matches. Note that
`pgvector` stores it as a distance by returning the negative value.

*When to use*: When embeddings are normalized and you want to rank by raw dot product (e.g.,
retrieval augmented generation setups using ANN).

## 3. Binary Vector Search (`embedding_binary` column)

Dense embeddings are also converted into 384-bit strings (one bit per dimension) so we can use
binary-friendly metrics. These strings are generated by the loader via `HybridSearch.Embeddings.Binary`
(default threshold `> 0.0`).

### Hamming distance (`<~>`)

*Implementation*: Custom SQL function `binary_hamming_distance/2`

*What it does*: Counts how many bit positions differ between two binary vectors.

*When to use*: Quick approximations, hashing, or when you want a cheap distance metric after
quantization.

### Jaccard distance (`<%>`)

*Implementation*: Custom SQL function `binary_jaccard_distance/2`

*What it does*: `1 - (|A ∧ B| / |A ∨ B|)`; considers both matches and mismatches. More expensive
than Hamming because it requires union/intersection bit counts.

*When to use*: When sparsity matters and you care about overlap rather than raw difference.

## Benchmark Snapshot

Command used: `mix bench.search --query "phoenix" --limit 5`

```
Name                     ips        average  deviation         median         99th %
vector l1             778.50        1.28 ms    ±41.36%        1.14 ms        3.11 ms
vector cosine         646.61        1.55 ms    ±46.79%        1.22 ms        3.65 ms
binary hamming        640.49        1.56 ms    ±48.26%        1.46 ms        4.00 ms
vector dot            527.90        1.89 ms    ±42.88%        1.82 ms        3.73 ms
vector l2             523.75        1.91 ms    ±39.92%        1.80 ms        4.01 ms
binary jaccard        292.49        3.42 ms    ±49.88%        3.19 ms        9.29 ms
bm25                  252.37        3.96 ms    ±53.81%        3.35 ms       13.76 ms
```

Observations:

- **Vector L1** was the fastest in this dataset, likely thanks to pgvector’s efficient L1 operator.
- **Cosine distance** and **binary Hamming** cluster close together—both are reasonable defaults.
- **Dot product** and **L2** are slower but still performant. They might be worth it if the metric
  better matches your application.
- **Binary Jaccard** is noticeably slower due to the additional bitwise operations.
- **BM25** trades throughput for lexical relevance and snippet generation—perfect when keyword
  matching is essential.

## Practical Tips

- Run `mix hybrid_search.load` to populate data before benchmarking.
- `mix bench.search` reuses the same embedding for every scenario to eliminate model overhead.
- For production use, consider enabling EXLA (GPU/CPU acceleration) so embedding generation stays
  fast during ingestion.
- IVFFlat indexes are created for all supported operators, but they require `ANALYZE` and a large
  dataset to shine. With few rows, sequential scans can outperform the index.

## Conclusions

- BM25 remains king for pure keyword search.
- Dense vectors (cosine or L1) are the most balanced semantic option.
- Binary distances are a lightweight approximation—useful for heuristics or as a cheaper secondary
  filter.

Feel free to tweak thresholds, embedding models, or index parameters (`lists`, `probes`) to further
optimize your use case.
